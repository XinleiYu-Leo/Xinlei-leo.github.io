---
permalink: /project/
title: "Research and Projects"
layout: "collection"
---

# Presentation and Demo

|:------------------:|:---------------------------|
| ![image1][SCR_Pre_Image]          | **Exploring Electrotactile Stimulation as a Modality for Sensation Illusion on the Arm:** Presented at the recent [SCR'23](https://sites.uci.edu/scr2023/schedule/)       <br> [[abstract](https://bpb-us-e2.wpmucdn.com/sites.uci.edu/dist/2/5230/files/2023/09/66_SCR_23_Xinlei_Yu.pdf)] |
| [![3T Demo](https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/3T_Demo_DARE-Cover.jpg)](https://youtu.be/6PznLd5wy5c "3T Demo")         | <strong>Tummy Time Toy</strong>: An Infant Learning Toy      <br> Demo at NSF DARE'23                 <br> [[video](https://youtu.be/6PznLd5wy5c)] |

[SCR_Pre_Image]: https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/talk_SCR.png "scr"


# Current Research Project 

|:------------------:|:---------------------------|
| <img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/sleeve_prototype.png" alt="Sleeve Image"/>             | I'm working on a haptic project that investigates the human perception and emotional state to <strong>electro-tactile stimulation</strong>. The further goal is to build a <strong>personalized</strong> and <strong>pleasant</strong> electro-tactile device that <strong>works seamlessly with other stimulus</strong> modalities such as visual and auditory.      |
| <img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/eeg_loop.png" alt="eeg Image"/>             | Currently, I'm working on a project that investigates enhancing EEG-based emotion recognition by applying Kalman filtering and smoothing techniques to EEG data from the public SEED dataset. <strong>The ultimate goal is to improve the evaluation of perceptual properties and human-computer interaction by enabling more precise quantification of human emotional states through EEG signal processing.</strong>     |



# Past Research Project 

|:---------------------:|:---------------------------|
|![electro_diagram][electro]|**Multi-armed bandit-based calibration for Electro-tactile Simulation:** Developed an electro-tactile display with a Sensory PCI card and a group of power sources and amplifiers and designed a multi-armed bandit-based calibration method to find an optimal signal parameter for pleasant stimulation. <br><br> [[Github(partially available)](https://github.com/xinleiyuUSC/MAB_UCB)] |
| ![image_3T][3T]               | **Tummy Time Toy:** A computer vision-based infant motor learning assistant toy (under US Patent review). This interactive toy rewards infants with lights and music when they lift their heads past a certain threshold, encouraging the development of prone motor skills. The primary goal is to study whether babies can learn to control their bodies during tummy time with the toy's assistance, aiding in muscle control and increasing their tolerance for tummy time.   <br><br> [[video](https://youtu.be/6PznLd5wy5c)] [Github(available soon)]  |

# Past Project

|:------------------:|:---------------------------|
| ![image_chatbot][haptics_chatbot]              | **Chatgpt-based chatbot with Haptics Knowledge base:** A **GPT-based chatbot** with a custom haptic research database using LangChain(call embedding model), LlamaIndex(vector indexing), OpenAI APIs, and gradio(lightweight UI). Provided researchers with a chatbot with the latest and custom database in the research domain and the power of chatGPT.  <br><br> [GitHub](https://github.com/xinleiyuUSC/haptics_chatBot)  |
| ![image_artsy][artsy]                | **Artsy App:** An Android and Web application for users to search for artists from Artsy’s database, look at detailed information about them(artwork image, bio, description, etc.)  <br><br> [GitHub Link](https://github.com/XinleiYu-Leo/Artsy_App)  |

[3T]: https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/TTT.png "3T Image"

[haptics_chatbot]: https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/chatBot.png "chatbot Image"

[electro]: https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/electro_diagram.png "Elec image"

[artsy]: https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/ArtsyImage.png "Artsy image"


[comment]: <> (
## 1. Ongoing Research Project: Wearable Electrotactile Rendering Device		    

I'm working on a haptic project that investigates the human perception of and emotional response to <strong>electro-tactile stimulation</strong>. The further goal is to build a <strong>personalized</strong> and <strong>pleasant</strong> electro-tactile device that <strong>works seamlessly with other stimulus</strong> modalities such as visual and auditory.

At the recent Southern California Robotics Symposium [SCR'23](https://sites.uci.edu/scr2023/schedule/), I had the honor of presenting our preliminary research on the effect of perception of electro-tactile stimulation. Our study examined how detection threshold (DT) and pain threshold (PT) are affected by the frequency and location of electro-tactile stimulation on the forearm. We found location significantly impacts DT and PT, with the ventral side more sensitive than the dorsal. Our findings emphasize the need for location-specific calibration when designing electro-tactile systems.

<img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/talk_SCR.png" alt="Sleeve Image" style="height: 500px; width:700px;"/>

Electro-tactile Sleeve prototype
<img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/sleeve_prototype.png" alt="Sleeve Image" style="height: 500px; width:500px;"/>


---


## 2. Research Project: Tummy Time Toy										 	                                                                   (July 2022 to April 2023) [Demo available!](https://youtu.be/6PznLd5wy5c)

Designed a **computer vision-based infant motor learning assistant toy**, aptly named the "Tummy Time Toy" (patent pending). This toy, similar to a play gym, **rewards infants** with lights and music when they lift their heads past a certain threshold, encouraging the development of **prone motor skills**. Its primary goal is to study if babies can learn to control their bodies when on their tummy, potentially assisting in **muscle control** and **increasing tummy time tolerance**. From a technical perspective, the toy is divided into two components: the front end, which offers user interaction, and the back end, which is responsible for computer vision, control logic, and various APIs. The toy and the data collected through studies will be part of a Ph.D. student's dissertation.  

While the toy is currently undergoing US patent evaluation and specific details remain confidential, I'm pleased to offer a brief demonstration showcased at the NSF DARE conference in 2023.

**Please check out the demo below by clicking the picture below**



---

## 3. Course/Research Project: EEG-based Emotion Recognition											 	                                                                  (October 2023 - Present)

Currently, I'm working on a project that investigates enhancing EEG-based emotion recognition by applying Kalman filtering and smoothing techniques to EEG data from the public SEED dataset. <strong>The ultimate goal is to improve the evaluation of perceptual properties and human-computer interaction by enabling more precise quantification of human emotional states through EEG signal processing.</strong>


---

## 4. Side Project: Haptic Knowledge Base chatbot											 	                                                                  (May 2023)

Built a **GPT-based chatbot** with a custom haptic research database using LangChain(call embedding model), LlamaIndex(vector indexing), OpenAI APIs, and gradio(lightweight UI). Provided researchers with a chatbot with the latest and custom database in the research domain and the power of chatGPT. [GitHub Link](https://github.com/xinleiyuUSC/haptics_chatBot)

<img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/chatBot.png" alt="Chat Bot Demo Image" style="height: 300px; width:900px;"/>

---

## 5. Course Project: Artsy Web/Android App											 	                                                                    (June 2022)

Built an Android mobile application for users to search for artists from Artsy’s database, look at detailed information about them(artwork image, bio, description, etc.) using various Artsy APIs, and add them to favorites for easy access. [GitHub Link](https://github.com/XinleiYu-Leo/Artsy_App)

The project is also on the Web version. Another version of backends written in Node.js is also available.

<img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/ArtsyImage.png" alt="Artsy Web Site Demo Image" style="height: 400px; width:700px;"/>
<img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/ArtsyImage2.png" alt="Artsy Web Site Demo Image" style="height: 400px; width:700px;"/>
)

# Undergraduate Project (2018-2021)
---

## 1. Senior Design Project: IoT Smart Light (Senior Design Project)[Website Link](https://sddec21-20.sd.ece.iastate.edu/)					                                                              (Jan 2021-Dec 2021)

Built a web-based User Interface leveraging Python, Django, and CSS for hosting lab simulations of cyberattacks on the power grid. Architected backends and databases utilizing Python and SQLite3 to provide database operations for 32 Zigbee lights data and query simulation data in binary from CVS files. Delivered Python APIs for Xbee coordinators to interact with UI and databases with XBee Python Library by transmitted package. The package includes field light ID, name, Mac address, and status. 

---

## 2. Course Project: Clustcore (Software Development Class)					                                                              (Aug 2019-Dec 2019)

Implemented frontend and UI for a bullet hell multiplayer game on Android devices in Java with Android Studio. Each user can sign up or log in with a pair of usernames and passwords to join a game. Devised multi-player game’s backend using SpringBoot with Rest API  and MySQL Database to contain user information, connections, and logs. Utilized Android Canvas API to implement all of the visualizations on the front-end, ObjectContainer to contain objects and characters in-game, and WebSocket and JSON to update the game in real time between server and clients/players. 

---
## 3. Course Project: Blockchain-enabled Flight Travel Insurance System		                                                         (April 2021-May 2021)

Deployed a dozen Ethereum smart contracts using Solidity with Remix IDE.  Users can purchase flight insurance policies and file claims based on given conditions. The smart contracts were deployed on the Ropsten Test Ethereum network with Metamask. Designed a  smart contract extension from a basic insurance contract. The insurance can verify claims using provided data (weather, locations, and time) and pay indemnities when passengers lose in test Ethers. 

---
## 4. Course Project: Autonomous Cleaning Robot							                                                                        (April 2019-May 2019)

Developed an iRobot Create cleaning robot using C with TI Launchpad that can autonomously navigate through a pipe maze and reach the designated destination, while mapping obstacles it encounters to a Graphic User Interface (Putty). Established a decision-making algorithm for choosing a moving direction based on calibrated radar and ultrasonic sensor input. 

<img src="https://raw.githubusercontent.com/XinleiYu-Leo/Xinlei-leo.github.io/master/assets/images/cleaning_robot.jpg" alt=" Demo Image" style="height: 400px; width:700px;"/>
---
